---
title: "2020 Rain Gauge Checks"
# author: "Zack Arno" 
output:
    # hide_code: true
  flexdashboard::flex_dashboard:
    vertical_layout: scroll
    orientation: columns
    # logo: "C:\\01_REACH_BGD\\02_GIS_DataUnit\\01_GIS_BASE_Data\\90_logos\\01_reach_impact_logos\\Logo_Reach_CMYK_2.jpg"
     
    # storyboard: true
---

```{r setup, include=FALSE,echo=F}
knitr::opts_chunk$set(fig.width=12, fig.height=8, 
                      echo=FALSE, warning=FALSE, message=FALSE)
library(flexdashboard)
library(tidyverse)
library(lubridate)
library(xts)
library(DT)
library(plotly)
# library(openair)


reporting_2020_files<-list.files(path = "01_data/02_weather_data/weather_data_2018_2020/Monthly 2020",pattern = "*.reporting.csv", full.names = T)
reporting_2020_files<-list.files(path = "01_data/02_weather_data/weather_data_2018_2020/Monthly 2020",pattern = "*.reporting.csv", full.names = T)
new_format_2020_files<-list.files(path = "01_data/02_weather_data/weather_data_2018_2020/Monthly 2020/",pattern = "*.xlsx", full.names = T)
new_format_2020_files<-list.files(path = "01_data/02_weather_data/weather_data_2018_2020/Monthly 2020/",pattern = "*.xlsx", full.names = T)

jan_may_2020<-map_dfr(reporting_2020_files, read_csv)
june_july_2020<-map_dfr(new_format_2020_files, readxl::read_xlsx,skip=5)
colnames(june_july_2020)<-c("Time","GSB Cox's Bazaar-1227", "UN Kuturc-1279", "UN Camp 16-1280",  "UN Chakmarkul-1278", "GSB Teknaf-1226")

jan_may_2020<- jan_may_2020 %>% 
  mutate(datetime=ymd_hms(Time)) %>% 
  select(-Time)

june_july_2020_long<- june_july_2020 %>% 
  pivot_longer(-Time) %>% 
  mutate(datetime=ymd_hms(Time),
         ts= str_sub(string = datetime,start = 12,end=19)) %>% 
  select(-Time)

colnames(june_july_2020_long)<-c("Device name", "Value","datetime")
all_2020_data<-bind_rows(jan_may_2020, june_july_2020_long)

all_2020_data<- all_2020_data %>% 
  mutate(date= as_date(datetime))

# These date have some issues
problem_dates<-ymd(c("2020-04-29", "2020-04-30", "2020-05-03"))
# data_all_dates %>% 
  # filter(date %in% problem_dates) %>% View()
all_2020_data<- all_2020_data %>%
  mutate(
    # Value=parse_number(Value),
    Value=ifelse((date  %in% problem_dates)& (Value<10|Value>100000),NA,Value)
  )

# all_2020_data %>% filter(date > "2020-07-01") %>% View()

# all_2020_data %>% 
  # filter(date %in% problem_dates) %>% View()


date_sequence<-seq(ymd(min(all_2020_data$date) ),ymd(max(all_2020_data$date)),by='days')
date_sequence_df<-data.frame(date_sequence) %>% select(date=date_sequence)
datetime_sequence<-seq(ymd_hms(paste(min(all_2020_data$date),"00:00:00")) ,ymd_hms(paste(max(all_2020_data$date)+1,"00:00:00")), by="hour")
datetime_sequence_df<-data.frame(datetime_sequence) %>% select(datetime=datetime_sequence)


# dates_missed_completely<-date_sequence[!date_sequence %in% all_2020_data$date]
# data_all_dates<- date_sequence_df %>% left_join(all_2020_data)
all_dates_with_gauge<-expand.grid(date_sequence_df$date, all_2020_data$`Device name` %>% unique())
all_datehour_with_gauge<-expand.grid(datetime_sequence_df$datetime, all_2020_data$`Device name` %>% unique())
colnames(all_dates_with_gauge)<-c("date", "Device name")
colnames(all_datehour_with_gauge)<-c("datehour", "Device name")

# i can join the date data to data  at this stage because we have a date column
# for hourly analysis you have to wait until after it is aggregated (because there is too much varaiation in the datetimes)
data_all_dates<- all_dates_with_gauge %>% left_join(all_2020_data)




data_by_day<- data_all_dates %>% 
  group_by(date,`Device name`,.drop=F) %>% 
  summarise(percent_day_working=sum(!is.na(Value))/n()) 

suppressWarnings(daily_accumulation<- data_all_dates %>% 
  group_by(date,`Device name`,.drop=F) %>% 
  summarise(mm= max(Value,na.rm = T)-min(Value,na.rm=T),
            mm= ifelse(mm==-Inf,NA,mm)))



all_2020_data_with_hour<-all_2020_data %>% 
  mutate(hr= str_sub(string = datetime,start = 12,end = 13),
         datehour= ymd_hms(paste(date, hr,":00:00")))

data_all_datehours<- all_datehour_with_gauge %>% left_join(all_2020_data_with_hour)

data_by_hour<- data_all_datehours %>% 
  group_by(datehour,`Device name`,.drop=F) %>% 
  summarise(percent_hour_working=sum(!is.na(Value))/n()) 


#for both days and hours  i can calculate the number of useable days
useable_days_per_month<-data_all_dates %>% 
   group_by(date,`Device name`,.drop=T) %>% 
  summarise(useable_day= ifelse(sum(!is.na(Value))>1,1,0)) %>% 
  group_by(datemonth=month(date),`Device name`,.drop=F) %>% 
  summarise(useable_day_per_month=sum(useable_day))

# data_all_datehours %>% 
#   group_by(month(datehour),`Device name`,.drop=F) %>% 
#   mutate(ts_gte2= sum(!is.na(Value))) %>% 
#   summarise(percent_hour_working=sum(!is.na(Value))/n(),
#             day_hour_useable= (ts_gte2>=2,na.rm=T)/n()
#             ) 


suppressWarnings(hourly_accumulation<- data_all_datehours  %>% 
  group_by(datehour,`Device name`,.drop=T) %>% 
  summarise(mm= max(Value,na.rm=T)-min(Value,na.rm=T),
            mm= ifelse(mm==-Inf,NA,mm)) )





```
Records/Gauge/Day
================================
This report is a short investigation of 2020 Rain gauge data.

#### Check daily counts 

- One of the first exploratory checks is to visualize a historgram of daily counts per gauge.
- Since measurements are taken at 15 minutes intervals each rain gauge should have 96 measurements per day.
- Immediately we can se that there were many days with 0 counts for all gauges. 

### Histogram - Number measurements per day per instrument
```{r}

daily_brks<-seq(0,480,96)
p1<-data_all_dates %>% 
  group_by(date,`Device name`) %>%
  summarise(num_records=n()) %>%
  ungroup() %>% 
  ggplot(aes(x=num_records, fill=`Device name`,label=num_records))+geom_histogram(position= "dodge")+
  scale_x_continuous(breaks = daily_brks)+ labs(x="# Daily Record",x="Frequency" )
# plotly::ggplotly()


example_bad_gauge<- data_all_dates %>% filter(date=="2020-03-19" ,`Device name`=="GSB Teknaf-1226" )

p1
# ggplotly(p1) %>% layout(margin = list(b = 90))
```



### Example where Teknaf gauge has way too many records for a day
- we see alot of 0 counts as well as values clustering at multiples of 96. This indicates a problem during the compilation of the data. It seems that records were likely double counted or time values entered incorrectly
- below is an example showing this issue. On 19 March 2020 the GSB Teknaf gauge had `r example_bad_gauge %>% nrow()` records. This is a problem because there should only be 96 per day. If we look at first couple records of the data you can see that we have gauge records for every minute. This is an error.\n


```{r}

# ggplotly(p1)
example_bad_gauge<-example_bad_gauge %>% 
  select(datetime,date:Rendered)

DT::datatable(example_bad_gauge)
````

Number useable days by month and gauge
================================

#### Basic Explanation of Issue and Data Exploration
- Since the rain gauges take a measurement every 15 minutes, there should be 96 valid measurements per day. To aggregate at any time step you need to calculate the difference in measurement throughout that time step. 
- With 15 minutes measurements we should be able to see the data every 15 mintues and aggreate it to 1-hourly, 3-hourly, daily etc. 
- However, we see there are large numbers of missing measurements. This is problematic.
- With just two measurements in a day it is theoretically possible to calculate the accumulated difference. This is not a great idea since we could have just the first 2 measuremets (half hour) of the day. The accumulation in this half hour would not be reflective of the entire days accumulation
- Nonetheless, for exploratory purposes I have looked into the nuber of days per month with 2 or more valid measurements per gauge. The results are below.
- If we did applied a criteria tha the two measurements have to be 10 hours apart we would have many less.

### Number days with more than 1 measurements per day (per gauge)
```{r}

# data_by_hour
p3<-useable_days_per_month %>% ungroup() %>% 
  ggplot(aes(x=datemonth, y=useable_day_per_month, fill=`Device name`))+
  geom_bar(stat="identity",position="dodge")+
  ggtitle("# of useable days per month") +
  labs(y= "# of useable days per month", x="month (2020)")+
    scale_x_continuous(breaks = seq(1,7,1))

p3
# plotly::ggplotly(p3, dynamicTicks = T) 

  # scale_x_date()
  # scale_y_continuous(labels= scales::percent),dynamicTicks=T)

```

  
Daily Accumulation attempt
================================
### Daily Accumulation By Rain Gauge
- With the current data it becomes apparent that it is difficult to make meaningful timeseries even for daily accumulation
- shorter timesteps like 1-hourly, or 3-hourly would be even more difficult.

```{r}
p<-daily_accumulation%>% 
  ggplot(aes(x=date, y=mm))+geom_path()+facet_wrap(~`Device name`)+
  theme(
    axis.text.x = element_text(angle=45),
    axis.title.x = element_blank()
  )
plotly::ggplotly(p, dynamicTicks = T) 

gauges_recording_in_2020_monsoon<-c("UN Camp 16-1280", "GSB Teknaf-1226")

daily_accumulation_wide<- daily_accumulation %>% 
  # filter(`Device name` %in% gauges_recording_in_2020_monsoon) %>% 
    add_rownames() %>% 
    pivot_wider(names_from = "Device name" , values_from = "mm") %>% 
    select(-rowname)

daily_accumulation_wide_xts<-xts(daily_accumulation_wide,daily_accumulation_wide$date)

```









```{r,eval=F}

# data_by_hour
plotly::ggplotly(data_by_hour %>% 
  ggplot(aes(x=datehour, y=percent_hour_working))+geom_bar(stat="identity")+facet_wrap(~`Device name`)+
    scale_y_continuous(labels=scales::percent)+
  ggtitle("% measurements per hour that functioned") +
  labs(y= "% measurements per hour that functioned"), dynamicTicks=T)



```

Row {data-width=700}
-------------------------------------


Row {data-width=700}
-------------------------------------

```{r,eval=F}
# data_by_hour
plotly::ggplotly(data_by_day %>% 
  ggplot(aes(x=date, y=percent_day_working))+geom_bar(stat="identity")+facet_wrap(~`Device name`)+
  ggtitle("% of Records Per Day Which Have a Value") %>% 
  labs(y= "% Functional Records")+
  scale_y_continuous(labels= scales::percent),dynamicTicks=T)

```






<!-- ### Daily asdfasd -->

```{r,eval=F}
library(dygraphs)
dygraph(daily_accumulation_wide_xts) %>% 
    dyOptions(connectSeparatedPoints = TRUE)



```


<!-- ### Hourly Accumulation -->


```{r,eval=F}
fig.path='Figs/'}

plotly::ggplotly(hourly_accumulation %>% 
  ggplot(aes(x=datehour, y=mm))+geom_path()+facet_wrap(~`Device name`), dynamicTicks=T)

```
